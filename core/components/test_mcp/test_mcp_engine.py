"""
Test MCP Engine - ClaudEditorè‡ªåŠ¨åŒ–æµ‹è¯•å¼•æ“
è´Ÿè´£åè°ƒå’Œæ‰§è¡Œæ‰€æœ‰æµ‹è¯•å¥—ä»¶
"""

import asyncio
import json
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional

from .test_suites.core_functionality_tests import CoreFunctionalityTests
from .test_suites.ai_functionality_tests import AIFunctionalityTests
from .test_suites.ui_integration_tests import UIIntegrationTests
from .test_suites.performance_tests import PerformanceTests
from .test_suites.compatibility_tests import CompatibilityTests
from .test_suites.security_tests import SecurityTests
from .reports.test_report_generator import TestReportGenerator
from .config.test_config import TestConfig


class TestMCPEngine:
    """Test MCPæ ¸å¿ƒå¼•æ“"""
    
    def __init__(self, config_path: Optional[str] = None):
        """åˆå§‹åŒ–æµ‹è¯•å¼•æ“"""
        self.config = TestConfig(config_path)
        self.report_generator = TestReportGenerator()
        
        # åˆå§‹åŒ–æµ‹è¯•å¥—ä»¶
        self.test_suites = {
            'core': CoreFunctionalityTests(self.config),
            'ai': AIFunctionalityTests(self.config),
            'ui': UIIntegrationTests(self.config),
            'performance': PerformanceTests(self.config),
            'compatibility': CompatibilityTests(self.config),
            'security': SecurityTests(self.config)
        }
        
        # æµ‹è¯•ç»“æœå­˜å‚¨
        self.test_results = {}
        self.start_time = None
        self.end_time = None
        
    async def run_release_testing(self, release_info: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ‰§è¡Œå®Œæ•´çš„å‘å¸ƒæµ‹è¯•æµç¨‹
        
        Args:
            release_info: å‘å¸ƒä¿¡æ¯
                {
                    'version': '4.4.0',
                    'platform': 'mac',
                    'release_type': 'minor',
                    'test_level': 'full'
                }
        
        Returns:
            æµ‹è¯•ç»“æœå­—å…¸
        """
        print(f"ğŸš€ å¼€å§‹ClaudEditor {release_info['version']} å‘å¸ƒæµ‹è¯•")
        print(f"ğŸ“‹ æµ‹è¯•å¹³å°: {release_info['platform']}")
        print(f"ğŸ¯ æµ‹è¯•çº§åˆ«: {release_info['test_level']}")
        
        self.start_time = datetime.now()
        
        try:
            # æ ¹æ®æµ‹è¯•çº§åˆ«é€‰æ‹©æµ‹è¯•å¥—ä»¶
            selected_suites = self._select_test_suites(release_info['test_level'])
            
            # å¹¶è¡Œæ‰§è¡Œæµ‹è¯•å¥—ä»¶
            results = await self._execute_test_suites(selected_suites, release_info)
            
            # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
            test_report = await self._generate_test_report(results, release_info)
            
            # è´¨é‡é—¨ç¦æ£€æŸ¥
            quality_gate_result = self._check_quality_gate(results)
            
            self.end_time = datetime.now()
            
            final_result = {
                'release_info': release_info,
                'test_results': results,
                'test_report': test_report,
                'quality_gate': quality_gate_result,
                'execution_time': (self.end_time - self.start_time).total_seconds(),
                'timestamp': self.end_time.isoformat()
            }
            
            # ä¿å­˜æµ‹è¯•ç»“æœ
            await self._save_test_results(final_result)
            
            print(f"âœ… æµ‹è¯•å®Œæˆï¼Œæ€»è€—æ—¶: {final_result['execution_time']:.2f}ç§’")
            print(f"ğŸ¯ è´¨é‡é—¨ç¦: {'é€šè¿‡' if quality_gate_result['passed'] else 'å¤±è´¥'}")
            
            return final_result
            
        except Exception as e:
            print(f"âŒ æµ‹è¯•æ‰§è¡Œå¤±è´¥: {str(e)}")
            raise
    
    def _select_test_suites(self, test_level: str) -> List[str]:
        """æ ¹æ®æµ‹è¯•çº§åˆ«é€‰æ‹©æµ‹è¯•å¥—ä»¶"""
        suite_configs = {
            'smoke': ['core'],  # å†’çƒŸæµ‹è¯•
            'regression': ['core', 'ai', 'ui'],  # å›å½’æµ‹è¯•
            'full': ['core', 'ai', 'ui', 'performance', 'compatibility', 'security'],  # å®Œæ•´æµ‹è¯•
            'performance': ['performance'],  # æ€§èƒ½æµ‹è¯•
            'security': ['security']  # å®‰å…¨æµ‹è¯•
        }
        
        return suite_configs.get(test_level, ['core'])
    
    async def _execute_test_suites(self, selected_suites: List[str], release_info: Dict[str, Any]) -> Dict[str, Any]:
        """å¹¶è¡Œæ‰§è¡Œæµ‹è¯•å¥—ä»¶"""
        results = {}
        
        # åˆ›å»ºå¹¶å‘ä»»åŠ¡
        tasks = []
        for suite_name in selected_suites:
            if suite_name in self.test_suites:
                task = asyncio.create_task(
                    self._run_single_test_suite(suite_name, release_info),
                    name=f"test_suite_{suite_name}"
                )
                tasks.append(task)
        
        # ç­‰å¾…æ‰€æœ‰æµ‹è¯•å®Œæˆ
        completed_tasks = await asyncio.gather(*tasks, return_exceptions=True)
        
        # å¤„ç†æµ‹è¯•ç»“æœ
        for i, result in enumerate(completed_tasks):
            suite_name = selected_suites[i]
            if isinstance(result, Exception):
                results[suite_name] = {
                    'status': 'error',
                    'error': str(result),
                    'passed': False,
                    'execution_time': 0
                }
            else:
                results[suite_name] = result
        
        return results
    
    async def _run_single_test_suite(self, suite_name: str, release_info: Dict[str, Any]) -> Dict[str, Any]:
        """è¿è¡Œå•ä¸ªæµ‹è¯•å¥—ä»¶"""
        print(f"ğŸ§ª è¿è¡Œ {suite_name} æµ‹è¯•å¥—ä»¶...")
        
        start_time = time.time()
        test_suite = self.test_suites[suite_name]
        
        try:
            # æ‰§è¡Œæµ‹è¯•å¥—ä»¶
            result = await test_suite.run(release_info)
            
            execution_time = time.time() - start_time
            
            # æ ‡å‡†åŒ–ç»“æœæ ¼å¼
            standardized_result = {
                'suite_name': suite_name,
                'status': 'completed',
                'passed': result.get('passed', False),
                'total_tests': result.get('total_tests', 0),
                'passed_tests': result.get('passed_tests', 0),
                'failed_tests': result.get('failed_tests', 0),
                'skipped_tests': result.get('skipped_tests', 0),
                'execution_time': execution_time,
                'details': result.get('details', {}),
                'metrics': result.get('metrics', {}),
                'errors': result.get('errors', [])
            }
            
            print(f"âœ… {suite_name} æµ‹è¯•å®Œæˆ: {standardized_result['passed_tests']}/{standardized_result['total_tests']} é€šè¿‡")
            
            return standardized_result
            
        except Exception as e:
            execution_time = time.time() - start_time
            print(f"âŒ {suite_name} æµ‹è¯•å¤±è´¥: {str(e)}")
            
            return {
                'suite_name': suite_name,
                'status': 'error',
                'passed': False,
                'total_tests': 0,
                'passed_tests': 0,
                'failed_tests': 1,
                'skipped_tests': 0,
                'execution_time': execution_time,
                'details': {},
                'metrics': {},
                'errors': [str(e)]
            }
    
    async def _generate_test_report(self, results: Dict[str, Any], release_info: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        return await self.report_generator.generate_report(results, release_info)
    
    def _check_quality_gate(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """æ£€æŸ¥è´¨é‡é—¨ç¦"""
        total_tests = sum(result.get('total_tests', 0) for result in results.values())
        passed_tests = sum(result.get('passed_tests', 0) for result in results.values())
        failed_tests = sum(result.get('failed_tests', 0) for result in results.values())
        
        # è®¡ç®—é€šè¿‡ç‡
        pass_rate = (passed_tests / total_tests * 100) if total_tests > 0 else 0
        
        # è´¨é‡é—¨ç¦è§„åˆ™
        quality_rules = {
            'min_pass_rate': 98.0,  # æœ€ä½é€šè¿‡ç‡98%
            'max_failed_tests': 2,  # æœ€å¤š2ä¸ªå¤±è´¥æµ‹è¯•
            'required_suites': ['core'],  # å¿…é¡»é€šè¿‡çš„æµ‹è¯•å¥—ä»¶
            'performance_requirements': {
                'startup_time': 3.0,  # å¯åŠ¨æ—¶é—´ < 3ç§’
                'memory_usage': 200,  # å†…å­˜ä½¿ç”¨ < 200MB
                'cpu_usage': 5.0  # CPUä½¿ç”¨ < 5%
            }
        }
        
        # æ£€æŸ¥è§„åˆ™
        gate_checks = {
            'pass_rate_check': pass_rate >= quality_rules['min_pass_rate'],
            'failed_tests_check': failed_tests <= quality_rules['max_failed_tests'],
            'required_suites_check': all(
                results.get(suite, {}).get('passed', False) 
                for suite in quality_rules['required_suites']
            ),
            'performance_check': self._check_performance_requirements(
                results, quality_rules['performance_requirements']
            )
        }
        
        # æ€»ä½“é€šè¿‡çŠ¶æ€
        overall_passed = all(gate_checks.values())
        
        return {
            'passed': overall_passed,
            'pass_rate': pass_rate,
            'total_tests': total_tests,
            'passed_tests': passed_tests,
            'failed_tests': failed_tests,
            'checks': gate_checks,
            'rules': quality_rules,
            'recommendation': self._generate_quality_recommendation(gate_checks, pass_rate)
        }
    
    def _check_performance_requirements(self, results: Dict[str, Any], requirements: Dict[str, float]) -> bool:
        """æ£€æŸ¥æ€§èƒ½è¦æ±‚"""
        performance_result = results.get('performance', {})
        if not performance_result.get('passed', False):
            return False
        
        metrics = performance_result.get('metrics', {})
        
        checks = [
            metrics.get('startup_time', float('inf')) <= requirements['startup_time'],
            metrics.get('memory_usage', float('inf')) <= requirements['memory_usage'],
            metrics.get('cpu_usage', float('inf')) <= requirements['cpu_usage']
        ]
        
        return all(checks)
    
    def _generate_quality_recommendation(self, checks: Dict[str, bool], pass_rate: float) -> str:
        """ç”Ÿæˆè´¨é‡å»ºè®®"""
        if all(checks.values()):
            return "âœ… æ‰€æœ‰è´¨é‡æ£€æŸ¥é€šè¿‡ï¼Œå¯ä»¥è¿›è¡Œéƒ¨ç½²"
        
        failed_checks = [check for check, passed in checks.items() if not passed]
        
        recommendations = {
            'pass_rate_check': f"âŒ æµ‹è¯•é€šè¿‡ç‡ {pass_rate:.1f}% ä½äºè¦æ±‚çš„98%ï¼Œéœ€è¦ä¿®å¤å¤±è´¥çš„æµ‹è¯•",
            'failed_tests_check': "âŒ å¤±è´¥æµ‹è¯•æ•°é‡è¶…è¿‡é™åˆ¶ï¼Œéœ€è¦ä¿®å¤å…³é”®é—®é¢˜",
            'required_suites_check': "âŒ æ ¸å¿ƒæµ‹è¯•å¥—ä»¶æœªé€šè¿‡ï¼Œå¿…é¡»ä¿®å¤åæ‰èƒ½å‘å¸ƒ",
            'performance_check': "âŒ æ€§èƒ½æµ‹è¯•æœªè¾¾æ ‡ï¼Œéœ€è¦ä¼˜åŒ–æ€§èƒ½æŒ‡æ ‡"
        }
        
        return "; ".join(recommendations[check] for check in failed_checks)
    
    async def _save_test_results(self, results: Dict[str, Any]) -> None:
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        # åˆ›å»ºæŠ¥å‘Šç›®å½•
        reports_dir = Path(__file__).parent / "reports" / "test_results"
        reports_dir.mkdir(parents=True, exist_ok=True)
        
        # ç”Ÿæˆæ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        version = results['release_info']['version']
        platform = results['release_info']['platform']
        
        filename = f"test_results_{version}_{platform}_{timestamp}.json"
        filepath = reports_dir / filename
        
        # ä¿å­˜JSONæ ¼å¼ç»“æœ
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"ğŸ“„ æµ‹è¯•ç»“æœå·²ä¿å­˜: {filepath}")
    
    async def run_smoke_test(self, release_info: Dict[str, Any]) -> Dict[str, Any]:
        """è¿è¡Œå¿«é€Ÿå†’çƒŸæµ‹è¯•"""
        release_info['test_level'] = 'smoke'
        return await self.run_release_testing(release_info)
    
    async def run_performance_test(self, release_info: Dict[str, Any]) -> Dict[str, Any]:
        """è¿è¡Œæ€§èƒ½æµ‹è¯•"""
        release_info['test_level'] = 'performance'
        return await self.run_release_testing(release_info)
    
    async def run_security_test(self, release_info: Dict[str, Any]) -> Dict[str, Any]:
        """è¿è¡Œå®‰å…¨æµ‹è¯•"""
        release_info['test_level'] = 'security'
        return await self.run_release_testing(release_info)
    
    def get_test_history(self, version: Optional[str] = None, platform: Optional[str] = None) -> List[Dict[str, Any]]:
        """è·å–æµ‹è¯•å†å²è®°å½•"""
        reports_dir = Path(__file__).parent / "reports" / "test_results"
        
        if not reports_dir.exists():
            return []
        
        history = []
        for file_path in reports_dir.glob("test_results_*.json"):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    result = json.load(f)
                
                # è¿‡æ»¤æ¡ä»¶
                if version and result['release_info']['version'] != version:
                    continue
                if platform and result['release_info']['platform'] != platform:
                    continue
                
                history.append(result)
            except Exception as e:
                print(f"âš ï¸ è¯»å–æµ‹è¯•å†å²æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
        
        # æŒ‰æ—¶é—´æˆ³æ’åº
        history.sort(key=lambda x: x['timestamp'], reverse=True)
        return history

